# RL Training Configuration for Math Tool-Use (ReTool style)

model:
  name_or_path: "Qwen/Qwen2.5-0.5B-Instruct"
  dtype: "bfloat16"
  attn_implementation: "sdpa"
  trust_remote_code: true
  compile: true
  lora:
    enabled: true
    r: 16
    alpha: 32
    dropout: 0.05
    target_modules:
      - q_proj
      - v_proj

training:
  algorithm: "grpo"  # GRPO is stable in TRL
  learning_rate: 1e-6
  batch_size: 256
  gradient_accumulation_steps: 1
  num_train_epochs: 1
  sync_weights: true
  max_seq_length: 1024
  report_to: "none"
  clip_range_ratio: [0.8, 1.28]

rollout:
  temperature: 0.6
  max_turns: 5
  top_p: 0.95
  top_k: 50

output_dir: "./output/rl"
seed: 42
logging_steps: 5
save_steps: 25
